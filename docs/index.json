[
{
	"uri": "http://cassandra-reaper.io/usage/add_cluster/",
	"title": "Adding a Cluster",
	"tags": [],
	"description": "",
	"content": "Enter an address of one of the nodes in the cluster, then click Add Cluster Reaper will contact that node and find the rest of the nodes in the cluster automatically.\n      Once successfully completed, the Cluster\u0026rsquo;s health will be displayed.\nIf JMX authentication is required and all clusters share the same credentials, they have to be filled in the Reaper YAML file, under jmxAuth (see the configuration reference).\nSpecific JMX credentials per cluster Since 1.1.0\nIf the clusters require authentication for JMX access, credentials will need to be filled in the reaper yaml configuration file. See the jmxCredentials setting in the configuration reference for detailed informations.\n      When using jmxCredentials (if each cluster has specific credentials), the name of the cluster will need to be indicated in the seed node address.\n"
},
{
	"uri": "http://cassandra-reaper.io/install/",
	"title": "Build and Install",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://cassandra-reaper.io/",
	"title": "Cassandra Reaper",
	"tags": [],
	"description": "",
	"content": "Overview Reaper is an open source tool that aims to schedule and orchestrate repairs of Apache Cassandra clusters.\nIt improves the existing nodetool repair process by\n Splitting repair jobs into smaller tunable segments. Handling back-pressure through monitoring running repairs and pending compactions. Adding ability to pause or cancel repairs and track progress precisely.  Reaper ships with a REST API, a command line tool and a web UI.\nThis documentation includes instructions on how build, install, and configure Reaper correctly.\nCompatibility Reaper supports all versions of Apache Cassandra ranging from 1.2 to 3.11. Incremental repair is supported for versions from 2.1 and above.\nA single instance of Reaper can handle repairs for clusters running different Apache Cassandra versions.\nDownload and Installation Head over to the Downloads section to download the version of Reaper that is compatible with your system.\n"
},
{
	"uri": "http://cassandra-reaper.io/downloads/",
	"title": "Downloads and Installation",
	"tags": [],
	"description": "",
	"content": "Packages The current stable version can be downloaded in the following packaging formats :\n  Deb  RPM  Tarball  The current development version can be downloaded in the following packaging formats :\n  Deb  RPM  Tarball  Quick Installation Guide   For a docker image, please see the Docker section.\nOnce the appropriate package has been downloaded, head over to the Install and Run section.\nUpgrade Guides @TODO: Add link to upgrade guides\n"
},
{
	"uri": "http://cassandra-reaper.io/metrics/graphite/",
	"title": "Graphite Reporter",
	"tags": [],
	"description": "",
	"content": "Reaper can be configured to periodically report metrics to a Graphite host. This can be done using the following properties in the Reaper configuration YAML file.\nMetrics: frequency: 1 minute reporters: - type: graphite host: \u0026lt;host_address\u0026gt; port: \u0026lt;port_number\u0026gt; prefix: \u0026lt;prefix\u0026gt; Where:\n host_address is hostname of the Graphite server to report to. port_number is port of the Graphite server to report to. prefix is prefix for Metric key names that are reported to the Graphite server. Typically this will be the hostname sending the metrics to the Graphite server.  "
},
{
	"uri": "http://cassandra-reaper.io/backends/memory/",
	"title": "In-Memory Backend",
	"tags": [],
	"description": "",
	"content": "To use in memory storage as the storage type for Reaper, the storageType setting must be set to memory in the Reaper configuration YAML file. Note that the in memory storage is enabled by default. An example of how to configure Reaper with In-Menory storage can be found in the cassandra-reaper-memory.yaml.\nstorageType: memory In-memory storage is volatile and as such all registered cluster, column families and repair information will be lost upon service restart. This storage setting is intended for testing purposes only.\n"
},
{
	"uri": "http://cassandra-reaper.io/install/install/",
	"title": "Package Install Guide",
	"tags": [],
	"description": "",
	"content": "After modifying the resource/cassandra-reaper.yaml config file, Reaper can be started using the following command line:\njava -jar target/cassandra-reaper-X.X.X.jar server resource/cassandra-reaper.yaml Once started, the UI can be accessed through: http://127.0.0.1:8080/webui/\nReaper can also be accessed using the REST API exposed on port 8080, or using the command line tool bin/spreaper\nInstalling and Running as a Service We provide prebuilt packages for reaper on the Bintray.\nRPM Install (CentOS, Fedora, RHEK) Grab the RPM from GitHub and install using the rpm command:\nsudo rpm -ivh reaper-*.*.*.x86_64.rpm Using yum (stable releases)  Run the following to get a generated .repo file:  wget https://bintray.com/thelastpickle/reaper-rpm/rpm -O bintray-thelastpickle-reaper-rpm.repo  OR - Copy this text into a bintray-thelastpickle-reaper-rpm.repo file on your Linux machine:  #bintraybintray-thelastpickle-reaper-rpm - packages by thelastpickle from Bintray [bintraybintray-thelastpickle-reaper-rpm] name=bintray-thelastpickle-reaper-rpm baseurl=https://dl.bintray.com/thelastpickle/reaper-rpm gpgcheck=0 repo_gpgcheck=0 enabled=1 Run the following command:  sudo mv bintray-thelastpickle-reaper-rpm.repo /etc/yum.repos.d/ Install reaper:  sudo yum install reaper Using yum (development builds)  Run the following to get a generated .repo file:  wget https://bintray.com/thelastpickle/reaper-rpm-beta/rpm -O bintray-thelastpickle-reaper-rpm-beta.repo  OR - Copy this text into a bintray-thelastpickle-reaper-rpm-beta.repo file on your Linux machine:  #bintraybintray-thelastpickle-reaper-rpm-beta - packages by thelastpickle from Bintray [bintraybintray-thelastpickle-reaper-rpm-beta] name=bintray-thelastpickle-reaper-rpm-beta baseurl=https://dl.bintray.com/thelastpickle/reaper-rpm-beta gpgcheck=0 repo_gpgcheck=0 enabled=1 Run the following command:  sudo mv bintray-thelastpickle-reaper-rpm-beta.repo /etc/yum.repos.d/ Install reaper:  sudo yum install reaper DEB (Debian based distros like Ubuntu) After downloading the DEB package, install using the dpkg command:\nsudo dpkg -i reaper_*.*.*_amd64.deb Using apt-get (stable releases)  Using the command line, add the following to your /etc/apt/sources.list system config file:  echo \u0026quot;deb https://dl.bintray.com/thelastpickle/reaper-deb wheezy main\u0026quot; | sudo tee -a /etc/apt/sources.list  OR - Add the repository URLs using the \u0026ldquo;Software Sources\u0026rdquo; admin UI:  deb https://dl.bintray.com/thelastpickle/reaper-deb wheezy main Install the public key:  sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 2895100917357435 Install reaper:  sudo apt-get update sudo apt-get install reaper Using apt-get (development builds)  Using the command line, add the following to your /etc/apt/sources.list system config file:  echo \u0026quot;deb https://dl.bintray.com/thelastpickle/reaper-deb-beta wheezy main\u0026quot; | sudo tee -a /etc/apt/sources.list  Or - Add the repository URLs using the \u0026ldquo;Software Sources\u0026rdquo; admin UI:  deb https://dl.bintray.com/thelastpickle/reaper-deb-beta wheezy main Install the public key:  sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 2895100917357435 Install reaper:  sudo apt-get update sudo apt-get install reaper Service Configuration The yaml file used by the service is located at /etc/cassandra-reaper/cassandra-reaper.yaml and alternate config templates can be found under /etc/cassandra-reaper/configs. It is recommended to create a new file with your specific configuration and symlink it as /etc/cassandra-reaper/cassandra-reaper.yaml to avoid your configuration from being overwritten during upgrades.\nAdapt the config file to suit your setup and then run:\nsudo service cassandra-reaper start Log files can be found at /var/log/cassandra-reaper.log and /var/log/cassandra-reaper.err.\nStop the service by running:\nsudo service cassandra-reaper stop "
},
{
	"uri": "http://cassandra-reaper.io/backends/h2/",
	"title": "H2 Backend",
	"tags": [],
	"description": "",
	"content": "To use H2 as the persistent storage for Reaper, the storageType setting must be set to h2 in the Reaper configuration YAML file. When using H2 storage, the database will be automatically created under the path configured in the configuration YAML file. An example of how to configure H2 as persistent storage for Reaper can be found in the cassandra-reaper-h2.yaml.\nstorageType: h2 h2: # H2 JDBC settings url: jdbc:h2:~/reaper-db/db;MODE=PostgreSQL user: password: "
},
{
	"uri": "http://cassandra-reaper.io/backends/postgres/",
	"title": "Postgres Backend",
	"tags": [],
	"description": "",
	"content": "To use PostgreSQL as the persistent storage for Reaper, the storageType setting must be set to postgres in the Reaper configuration YAML file. The schema will be initialized/upgraded automatically upon startup in the configured database. Ensure that the correct JDBC credentials are specified in the cassandra-reaper.yaml to allow object creation. An example of how to configure Postgres as persistent storage for Reaper can be found in the cassandra-reaper-postgres.yaml.\nstorageType: postgres postgres: # PostgreSQL JDBC settings user: postgres password: url: jdbc:postgresql://127.0.0.1/reaper "
},
{
	"uri": "http://cassandra-reaper.io/backends/cassandra/",
	"title": "Cassandra Backend",
	"tags": [],
	"description": "",
	"content": "To use Apache Cassandra as the persistent storage for Reaper, the storageType setting must be set to cassandra in the Reaper configuration YAML file. In addition, the connection details for the Apache Cassandra cluster being used to store Reaper data must be specified in the configuration YAML file. An example of how to configure Cassandra as persistent storage for Reaper can be found in the cassandra-reaper-cassandra.yaml.\nstorageType: cassandra cassandra: clusterName: \u0026#34;test\u0026#34; contactPoints: [\u0026#34;127.0.0.1\u0026#34;] keyspace: reaper_db queryOptions: consistencyLevel: LOCAL_QUORUM serialConsistencyLevel: SERIAL If you\u0026rsquo;re using authentication or SSL:\nstorageType: cassandra cassandra: clusterName: \u0026#34;test\u0026#34; contactPoints: [\u0026#34;127.0.0.1\u0026#34;] keyspace: reaper_db authProvider: type: plainText username: cassandra password: cassandra ssl: type: jdk The Apache Cassandra backend is the only deployment that allows multiple Reaper instances to operate concurrently. This provides high availability and allows to repair multi DC clusters.\nTo run Reaper using the Cassandra backend, create a reaper_db keyspace with an appropriate placement strategy. This is installation specific, and names of the data centers in the cluster that will host the Reaper data must be specified. For example:\nCREATE KEYSPACE reaper_db WITH replication = {'class': 'NetworkTopologyStrategy', '\u0026lt;data_center\u0026gt;': 3}; Where:\n \u0026lt;data_center\u0026gt; is the name of the Cassandra data center that will contain the keyspace replicas.  When operating Reaper in a production environment, it is recommended that:\n An RF (Replication Factor) of 3 be used in each data center for the reaper_db keyspace. This is to ensure that all Reaper state data is still available should a node in the cluster be unavailable. The NetworkTopologyStrategy should be used for the replication strategy of the keyspace. This is because LOCAL_* requests will fail if the SimpleNetworkingStrategy is used in an environment where there is more than one data center defined.  Schema initialization and migration will be done automatically upon startup.\n"
},
{
	"uri": "http://cassandra-reaper.io/configuration/reaper_specific/",
	"title": "Reaper Specific Settings",
	"tags": [],
	"description": "",
	"content": "Configuration settings in the cassandra-reaper.yaml that are specific to Reaper\nautoScheduling Optional setting to automatically setup repair schedules for all non-system keyspaces in a cluster. If enabled, adding a new cluster will automatically setup a schedule repair for each keyspace. Cluster keyspaces are monitored based on a configurable frequency, so that adding or removing a keyspace will result in adding / removing the corresponding scheduled repairs.\nautoScheduling: enabled: true initialDelayPeriod: PT15S periodBetweenPolls: PT10M timeBeforeFirstSchedule: PT5M scheduleSpreadPeriod: PT6H excludedKeyspaces: [myTTLKeyspace, ...]  Definitions for the above sub-settings are as follows.\nenabled Type: Boolean\nDefault: false\nEnables or disables autoScheduling.\ninitialDelayPeriod Type: String\nDefault: PT15S (15 seconds)\nThe amount of delay time before the schedule period starts.\nperiodBetweenPolls Type: String\nDefault: PT10M (10 minutes)\nThe interval time to wait before checking whether to start a repair task.\ntimeBeforeFirstSchedule Type: String\nDefault: PT5M (5 minutes)\nGrace period before the first repair in the schedule is started.\nscheduleSpreadPeriod Type: String\nDefault: PT6H (6 hours)\nThe time spacing between each of the repair schedules that is to be carried out.\nexcludedKeyspaces Type: Array (comma separated Strings)\nThe Keyspaces that are to be excluded from the repair schedule.\ndatacenterAvailability Type: String\nDefault: ALL\nIndicates to Reaper its deployment in relation to cluster data center network locality. The value must be either ALL, LOCAL, EACH or SIDECAR. Note that this setting controls the behavior for metrics collection.\nBy default, Apache Cassandra restricts JMX communications to localhost only. In this setup, only the SIDECAR value is suitable.\nFor security reasons, it is possible that Reaper will have access limited to nodes in a single datacenter via JMX (multi region clusters for example). In this case, it is possible to deploy an operate an instance of Reaper in each datacenter where each instance only has access via JMX (with or without authentication) to the nodes in its local datacenter. In addition, Reaper will check the number of pending compactions and actively running repairs on all replicas prior to processing a segment.\nALL - requires Reaper to have access via JMX to all nodes across all datacenters. In this mode Reaper can be backed by all available storage types.\nLOCAL - requires Reaper to have access via JMX to all nodes only in the same datacenter local to Reaper. A single Reaper instance can operate in this mode and trigger repairs from within its local data center. In this case, can be backed by all available storage types and repairs to any remote datacenters are be handled internally by Cassandra. A Reaper instance can be deployed to each datacenter and be configured to operate in this mode. In this case, Reaper can use either of Apache Cassandra or Postgres as its storage.\nFurther information can be found in the Operating with a Multi DC Cluster section.\nEACH - requires a minimum of one Reaper instance operating in each datacenter. Each Reaper instance is required to have access via JMX to all nodes only in its local datacenter. When operating in this mode, Reaper can use either of Apache Cassandra or Postgres as its storage. In addition, metrics from nodes in remote datacenters must be collected through the storage backend. If any metric is unavailable, the segment will be postponed for later processing.\nFurther information can be found in the Operating with a Multi DC Cluster section.\nSIDECAR - requires one reaper instance for each node in the cluster. Each Reaper instance is required to have access via JMX to its local node. When operating in this mode, Reaper can use either of Apache Cassandra or Postgres as its storage.\nFurther information can be found in the Sidecar Mode section.\nenableCrossOrigin Type: Boolean\nDefault: true\nOptional setting which can be used to enable the CORS headers for running an external GUI application, like this project. When enabled it will allow REST requests incoming from other origins than the domain that hosts Reaper.\nenableDynamicSeedList Type: Boolean\nDefault: true\nAllow Reaper to add all nodes in the cluster as contact points when adding a new cluster, instead of just adding the provided node.\nhangingRepairTimeoutMins Type: Integer\nThe amount of time in minutes to wait for a single repair to finish. If this timeout is reached, the repair segment in question will be cancelled, if possible, and then scheduled for later repair again within the same repair run process.\nincrementalRepair Type: Boolean\nDefault: false\nSets the default repair type unless specifically defined for each run. Note that this is only supported with the PARALLEL repairParallelism setting. For more details in incremental repair, please refer to the following article.http://www.datastax.com/dev/blog/more-efficient-repairs\nNote: It is recommended to avoid using incremental repair before Cassandra 4.0 as subtle bugs can lead to overstreaming and cluster instabililty.\nblacklistTwcsTables Type: Boolean\nDefault: false\nDisables repairs of any tables that use either the TimeWindowCompactionStrategy or DateTieredCompactionStrategy. This automatic blacklisting is not stored in schedules or repairs. It is applied when repairs are triggered and visible in the UI for running repairs. Not storing which tables are TWCS/DTCS ensures changes to a table\u0026rsquo;s compaction strategy are honored on every new repair.\nNote: It is recommended to enable this option as repairing these tables, when they contain TTL\u0026rsquo;d data, causes overlaps between partitions across the configured time windows the sstables reside in. This leads to an increased disk usage as the older sstables are unable to be expired despite only containing TTL\u0026rsquo;s data. Repairing DTCS tables has additional issues and is generally not recommended.\njmxAuth Optional setting to allow Reaper to establish JMX connections to Cassandra clusters using password based JMX authentication.\njmxAuth: username: cassandra password: cassandra #### `username`  username Type: String\nCassandra JMX username.\npassword Type: String\nCassandra JMX password.\njmxCredentials Since 1.1.0 Optional setting to allow Reaper to establish JMX connections to Cassandra clusters with specific credentials per cluster.\njmxCredentials: clusterProduction1: username: user1 password: password1 clusterProduction2: username: user2 password: password2  This setting can be used in conjunction with the jmxAuth to override the credentials for specific clusters only. The cluster name must match the one defined in the cassandra.yaml file (in the example above, clusterProduction1 and clusterProduction2).\nAdding a new cluster with specific credentials requires to add the seed node in the following format : host@cluster To match the example above, it could be something like : 10.0.10.5@clusterProduction1\njmxConnectionTimeoutInSeconds Type: Integer\nDefault: 20\nControls the timeout for establishing JMX connections. The value should be low enough to avoid stalling simple operations in multi region clusters, but high enough to allow connections under normal conditions.\njmxPorts Type: Object\nOptional mapping of custom JMX ports to use for individual hosts. The used default JMX port value is 7199. CCM users will find IP and port number to add in ~/.ccm/\u0026lt;cluster\u0026gt;/*/node.conf or by running ccm \u0026lt;node\u0026gt; show.\njmxPorts: 127.0.0.1: 7100 127.0.0.2: 7200 127.0.0.3: 7300  logging Settings to configure Reaper logging.\nlogging: level: INFO loggers: io.dropwizard: WARN org.eclipse.jetty: WARN appenders: - type: console logFormat: \u0026quot;%-6level [%d] [%t] %logger{5} - %msg %n\u0026quot; threshold: WARN - type: file logFormat: \u0026quot;%-6level [%t] %logger{5} - %msg %n\u0026quot; currentLogFilename: /var/log/cassandra-reaper/reaper.log archivedLogFilenamePattern: /var/log/cassandra-reaper/reaper-%d.log.gz archivedFileCount: 99  Definitions for some of the above sub-settings are as follows.\nlevel Type: String\nGlobal log level to filter to. Where the level order is ALL \u0026lt; DEBUG \u0026lt; INFO \u0026lt; WARN \u0026lt; ERROR \u0026lt; FATAL \u0026lt; OFF. See the log4j documentation for further information.\nloggers Type: Object\nKey value pair containing the logger class name as the key and other sub-settings as its value.\nlogFormat Type: String\nThe output format of an entry in the log.\nthreshold Type: String\nThe log level to filter the console messages to. Where the level order is ALL \u0026lt; DEBUG \u0026lt; INFO \u0026lt; WARN \u0026lt; ERROR \u0026lt; FATAL \u0026lt; OFF. See the log4j documentation for further information.\narchivedFileCount Type: Integer\nThe number of archive log files stored in the log rotation sliding window. That is the number of archived (compressed) log files kept at one point in time. If there are archivedFileCount number of archived log files and the current (uncompressed) log file is archived, the oldest archived log file is deleted.\nmetrics Type: Object\nConfiguration parameters for sending metrics to a reporting system via the Dropwizard interface. Cassandra Reaper ships the Graphite and DataDog reporters by default.\nFurther information on metrics configuration can be found in the Metrics section.\nrepairIntensity Type: Float (value between 0.0 and 1.0, but must never be 0.0.)\nRepair intensity defines the amount of time to sleep between triggering each repair segment while running a repair run. When intensity is 1.0, it means that Reaper doesn\u0026rsquo;t sleep at all before triggering next segment, and otherwise the sleep time is defined by how much time it took to repair the last segment divided by the intensity value. 0.5 means half of the time is spent sleeping, and half running. Intensity 0.75 means that 25% of the total time is used sleeping and 75% running. This value can also be overwritten per repair run when invoking repairs.\nrepairManagerSchedulingIntervalSeconds Type: Integer\nDefault: 30\nControls the pace at which the Repair Manager will schedule processing of the next segment. Reducing this value from its default value of 30s to a lower value can speed up fast repairs by orders of magnitude.\nrepairParallelism Type: String\nType of parallelism to apply by default to repair runs. The value must be either SEQUENTIAL, PARALLEL, or DATACENTER_AWARE.\nSEQUENTIAL - one replica at a time, validation compaction performed on snapshots\nPARALLEL - all replicas at the same time, no snapshot\nDATACENTER_AWARE - one replica in each DC at the same time, with snapshots. If this value is used in clusters older than 2.0.12, Reaper will fall back into using SEQUENTIAL for those clusters.\nrepairRunThreadCount Type: Integer\nThe amount of threads to use for handling the Reaper tasks. Have this big enough not to cause blocking in cause some thread is waiting for I/O, like calling a Cassandra cluster through JMX.\nscheduleDaysBetween Type: Integer\nDefault: 7\nDefines the amount of days to wait between scheduling new repairs. The value configured here is the default for new repair schedules, but you can also define it separately for each new schedule. Using value 0 for continuous repairs is also supported.\nsegmentCountPerNode Type: Integer\nDefault: 16\nDefines the default amount of repair segments to create for newly registered Cassandra repair runs, for each node in the cluster. When running a repair run by Reaper, each segment is repaired separately by the Reaper process, until all the segments in a token ring are repaired. The count might be slightly off the defined value, as clusters residing in multiple data centers require additional small token ranges in addition to the expected. This value can be overwritten when executing a repair run via Reaper.\nIn a 10 nodes cluster, setting a value of 20 segments per node will generate a repair run that splits the work into 200 token subranges. This number can vary due to vnodes (before 1.2.0, Reaper cannot create a segment with multiple token ranges, so the number of segments will be at least the total number of vnodes in the cluster). As Reaper tries to size segments evenly, the presence of very small token ranges can lead to have more segments than expected.\nserver Settings to configure the application UI server.\nserver: type: default applicationConnectors: - type: http port: 8080 bindHost: 0.0.0.0 adminConnectors: - type: http port: 8081 bindHost: 0.0.0.0 requestLog: appenders: []  Reaper provides two separate UIs; the application UI which is configured via the applicationConnectors settings and the administration UI which is configured via the adminConnectors settings. The application UI provides functionality to create/manage cluster schedules, and the administration UI provides tools for monitoring and debugging the Reaper system.\nport For the applicationConnectors this setting will be the port number used to access the application UI. For the adminConnectors this setting will be the port number to access the administration UI.\nNote that the port numbers for each must be different values when bound to the same host.\nbindHost For the applicationConnectors this setting will be the host address used to access the application UI. For the adminConnectors this setting will be the host address used to access the administration UI.\nNote that to bind the service to all interfaces use value 0.0.0.0 or leave the value for the setting this blank. A value of * is an invalid value for this setting.\nstorageType Type: String\nThe storage type to use in which Reaper will store its control data. The value must be either cassandra, h2, memory, or postgres. If the recommended (persistent) storage type cassandra, h2, or postgres is being used, the database client parameters must be specified in the respective cassandra, h2, or postgres section in the configuration file. See the example settings in provided the src/packaging/resources directory of the repository.\nuseAddressTranslator Type: Boolean\nDefault: false\nWhen running multi region clusters in AWS, turn this setting to true in order to use the EC2MultiRegionAddressTranslator from the Datastax Java Driver. This will allow translating the public address that the nodes broadcast to the private IP address that is used to expose JMX.\naccessControl Settings to activate and configure authentication for the web UI. Deleting or commenting that block from the yaml file will turn off authentication.\naccessControl: sessionTimeout: PT10M shiro: iniConfigs: [\u0026quot;file:/path/to/shiro.ini\u0026quot;] repairThreadCount Type: Integer\nSince Cassandra 2.2, repairs are multithreaded in order to process several token ranges concurrently and speed up the process. This setting allows to set a default for automatic repair schedules. No more than four threads are allowed by Cassandra.\nmaxPendingCompactions Type: Integer\nDefault: 20\nReaper will prevent repair from overwhelming the cluster when lots of SSTables are streamed, by pausing segment processing if there are more than a specific number of pending compactions. Adjust this setting if you have a lot of tables in the cluster and the total number of pending compactions is usually high.\n"
},
{
	"uri": "http://cassandra-reaper.io/concepts/",
	"title": "Core Concepts",
	"tags": [],
	"description": "",
	"content": "Segments Reaper splits repair runs in segments. A segment is a subrange of tokens that fits entirely in one of the cluster token ranges. The minimum number of segments for a repair run is the number of token ranges in the cluster. With a 3 nodes cluster using 256 vnodes per node, a repair run will have at least 768 segments. If necessary, each repair can define a higher number of segments than the number of token ranges.\nAs of Reaper 1.2.0 and Apache Cassandra 2.2, token ranges that have the same replicas can be consolidated into a single segment. If the total number of requested segments is lower than the number of vnodes, Reaper will try to group token ranges so that each segment has the appropriate number of tokens. In Cassandra 2.2, one repair session will be started for each subrange of the segment, so the gain will be the reduction of overhead in Reaper. Starting with 3.0, Cassandra will generate a single repair session for all the subranges that share the same replicas, which then further reduces the overhead of vnodes in Cassandra.\nBack-pressure Reaper will associate each segment with its replicas, and run repairs sequentially on only one of the replicas. If a repair is already running on one of the replicas or if there are more than 20 pending compactions, Reaper will postpone the segment for future processing and try to repair the next segment.\nConcurrency and Multithreading Being a multi threaded service, Reaper will compute how many concurrent repair sessions can run on the cluster and adjust its thread pool accordingly. To that end, it will check the number of nodes in the cluster and the RF (Replication Factor) of the repaired keyspace. On a three node cluster with RF=3, only one segment can be repaired at a time. On a six node cluster with RF=3, two segments can be repaired at the same time.\nThe maximum number of concurrent repairs is 15 by default and can be modified in the YAML configuration (cassandra-reaper.yaml) file.\nSince Cassandra 2.2, repairs are multithreaded in order to process several token ranges concurrently and speed up the process. No more than four threads are authorized by Cassandra. The number of repair threads can be set differently for each repair run/schedule. This setting will be ignored for clusters running an older version of Apache Cassandra.\nTimeout By default, each segment must complete within 30 minutes. Reaper subscribes to the repair service notifications of Cassandra to monitor completion, and if a segment takes more than 30 minutes it gets cancelled and postponed. This means that if a repair job is subject to frequent segment cancellation, it is necessary to either split it up into more segments or raise the timeout over its default value.\nPause and Resume Pausing a repair will force the termination of all running segments. Once the job is resumed, cancelled segments will be fully processed once again from the beginning.\nIntensity Intensity controls the eagerness by which Reaper triggers repair segments. The Reaper will use the duration of the previous repair segment to compute how much time to wait before triggering the next one. The idea behind this is that long segments mean a lot of data mismatch, and thus a lot of streaming and compaction. Intensity allows reaper to adequately back off and give the cluster time to handle the load caused by the repair.\nScheduling Interval Reaper polls for new segments to process at a fixed interval. By default the interval is set to 30 seconds and this value can be modified in the Reaper configuration YAML file.\n"
},
{
	"uri": "http://cassandra-reaper.io/install/docker/",
	"title": "Docker Setup Guide",
	"tags": [],
	"description": "",
	"content": "Docker and Docker Compose will need to be installed in order to use the commands in this section.\nBuilding Reaper Docker Image Prerequisite The generation of the Docker image requires that the JAR file be built and placed in the src/packages directory. If the JAR package is missing from the directory then it can built using either the steps in the Docker package build section (above), or in the Building from Source section.\nBuilding Image To build the Reaper Docker Image which is then added to the local image cache using the cassandra-reaper:latest tag, run the following command from the top level directory.\nmvn -pl src/server/ docker:build -Ddocker.directory=src/server/src/main/docker Docker Hub Image A prebuilt Docker Image is available for download from Docker Hub. The image TAG can be specified when pulling the image from Docker Hub to pull a particular version. Set:\n TAG=master to run Reaper with the latest commits TAG=latest to run Reaper with the latest stable release  To pull the image from Docker Hub with a particular tag, run the following command.\ndocker pull thelastpickle/cassandra-reaper:${TAG} Start Docker Environment Using Docker Reaper can be executed within a Docker container with either an ephemeral memory storage or persistent database.\nIn-Memory Backend To launch a Reaper container backed by an In-Memory backend, use the following example with the appropriate JMX authentication settings for the cluster it will manage repairs for.\nTAG=latest REAPER_JMX_AUTH_USERNAME=reaperUser REAPER_JMX_AUTH_PASSWORD=reaperPass docker run \\ -p 8080:8080 \\ -p 8081:8081 \\ -e \u0026quot;REAPER_JMX_AUTH_USERNAME=${REAPER_JMX_AUTH_USERNAME}\u0026quot; \\ -e \u0026quot;REAPER_JMX_AUTH_PASSWORD=${REAPER_JMX_AUTH_PASSWORD}\u0026quot; \\ thelastpickle/cassandra-reaper:${TAG} Then visit the the Reaper UI: http://localhost:8080/webui/.\nCassandra Backend To launch a Reaper container backed by Cassandra, use the following example to connect to a Cassandra cluster that already has the reaper_db keyspace. Set the appropriate JMX authentication settings for the cluster that Reaper will manage repairs for.\nTAG=latest REAPER_JMX_AUTH_USERNAME=reaperUser REAPER_JMX_AUTH_PASSWORD=reaperPass REAPER_CASS_CLUSTER_NAME=reaper-cluster REAPER_CASS_CONTACT_POINTS=[\u0026quot;192.168.2.185\u0026quot;] docker run \\ -p 8080:8080 \\ -p 8081:8081 \\ -e \u0026quot;REAPER_JMX_AUTH_USERNAME=${REAPER_JMX_AUTH_USERNAME}\u0026quot; \\ -e \u0026quot;REAPER_JMX_AUTH_PASSWORD=${REAPER_JMX_AUTH_PASSWORD}\u0026quot; \\ -e \u0026quot;REAPER_STORAGE_TYPE=cassandra\u0026quot; \\ -e \u0026quot;REAPER_CASS_CLUSTER_NAME=${REAPER_CASS_CLUSTER_NAME}\u0026quot; \\ -e \u0026quot;REAPER_CASS_CONTACT_POINTS=${REAPER_CASS_CONTACT_POINTS}\u0026quot; \\ -e \u0026quot;REAPER_CASS_KEYSPACE=reaper_db\u0026quot; \\ thelastpickle/cassandra-reaper:${TAG} Then visit the the Reaper UI: http://localhost:8080/webui/.\nUsing Docker Compose The Docker Compose services available allow for orchestration of an environment that uses Reaper\u0026rsquo;s default settings. This provides a quick way to start Reaper and become familiar with its usage without the need of additional infrastructure. The environment created using Docker Compose comprises a single containerised Apache Cassandra node and a single containerised Reaper service.\nIn addition to the environment using Reaper\u0026rsquo;s default settings, Docker Compose services are provided that allow orchestration of an environment in which the connections between Reaper and Cassandra are SSL encrypted. The services which create this environment contain a -ssl suffix in their name.\nAll available Docker Compose services can be found in the docker-compose.yml file.\nDefault Settings Environment From the top level directory change to the src/packaging directory:\ncd src/packaging Start the Cassandra cluster:\ndocker-compose up cassandra The nodetool Docker Compose service can be used to check on the Cassandra node\u0026rsquo;s status:\ndocker-compose run nodetool status Once the Cassandra node is online and accepting CQL connections, create the required reaper_db Cassandra keyspace to allow Reaper to save its cluster and scheduling data.\nBy default, the reaper_db keyspace is created using a replication factor of 1. To change this replication factor, provide the intended replication factor as an optional argument:\ndocker-compose run cqlsh-initialize-reaper_db [$REPLICATION_FACTOR] Wait a few moments for the reaper_db schema change to propagate, then start Reaper:\ndocker-compose up reaper SSL Encrypted Connections Environment From the top level directory change to the src/packaging directory:\ncd src/packaging Generate the SSL Keystore and Truststore which will be used to encrypt the connections between Reaper and Cassandra.\ndocker-compose run generate-ssl-stores Start the Cassandra cluster which encrypts both the JMX and Native Protocol:\ndocker-compose up cassandra-ssl The nodetool-ssl Docker Compose service can be used to check on the Cassandra node\u0026rsquo;s status:\ndocker-compose run nodetool-ssl status Once the Cassandra node is online and accepting encrypted SSL connections via the Native Transport protocol, create the required reaper_db Cassandra keyspace to allow Reaper to save its cluster and scheduling data.\nBy default, the reaper_db keyspace is created using a replication factor of 1. To change this replication factor, provide the intended replication factor as an optional argument:\ndocker-compose run cqlsh-initialize-reaper_db-ssl [$REPLICATION_FACTOR] Wait a few moments for the reaper_db schema change to propagate, then start the Reaper service that will establish encrypted connections to Cassandra:\ndocker-compose up reaper-ssl Access The Environment Once started, the UI can be accessed through:\nhttp://127.0.0.1:8080/webui/\nA nodetool Docker Compose service is included for both the default and SSL encrypted environments to allow nodetool commands to be performed on Cassandra.\nFor the default environment use:\ndocker-compose run nodetool status For the SSL encrypted environment use:\ndocker-compose run nodetool-ssl status When adding the Cassandra node to the Reaper UI, the above commands can be used to find the node IP address.\nA cqlsh Docker Compose service is included as well for both the default and SSL encrypted environments to allow the creation of user tables in Cassandra.\nFor the default environment use:\ndocker-compose run cqlsh For the SSL encrypted environment use:\ndocker-compose run cqlsh-ssl Destroying the Docker Environment When terminating the infrastructure, use the following command to stop all related Docker Compose services:\ndocker-compose down To completely clean up all persistent data, delete the ./data/ directory:\nrm -rf ./data/ "
},
{
	"uri": "http://cassandra-reaper.io/configuration/backend_specific/",
	"title": "Backend Specific Settings",
	"tags": [],
	"description": "",
	"content": "Configuration settings in the cassandra-reaper.yaml that are specific to a particular backend.\nCassandra Settings The following settings are specific to a Reaper deployment that is backed by an Apache Cassandra database. Note that Cassandra backend configuration relies on the Dropwizard-Cassandra module.\nactivateQueryLogger Type: Boolean\nDefault: false\nRecords the CQL calls made to the Cassandra backend in the log output.\ncassandra Settings to configure Reaper to use Cassandra for storage of its control data. Reaper uses the Cassandra Java driver version 3.1.4 to perform operations on the cluster. An example of the configuration settings for the driver are as follows.\ncassandra: clusterName: \u0026#34;test\u0026#34; contactPoints: [\u0026#34;127.0.0.1\u0026#34;] port: 9042 keyspace: reaper_db loadBalancingPolicy: type: tokenAware shuffleReplicas: true subPolicy: type: dcAwareRoundRobin localDC: usedHostsPerRemoteDC: 0 allowRemoteDCsForLocalConsistencyLevel: false authProvider: type: plainText username: cassandra password: cassandra Definitions for some of the above sub-settings are as follows.\nclusterName Type: String\nName of the cluster to use to store the Reaper control data.\ncontactPoints Type: Array (comma separated Strings)\nSeed nodes in the Cassandra cluster to contact.\n[\u0026#34;127.0.0.1\u0026#34;, \u0026#34;127.0.0.2\u0026#34;, \u0026#34;127.0.0.3\u0026#34;] port Type: Integer\nDefault: 9042\nCassandra\u0026rsquo;s native port to connect to.\nkeyspace Type: String\nName of the keyspace to store the Reaper control data.\nloadBalancingPolicy Settings to configure the policies used to generate the query plan which determines the nodes to connect to when performing query operations. Further information can be found in the Cassandra Java driver Load balancing section.\nsubPolicy Settings to configure a child policy which used if the initial policy fails to determine a node to connect to.\ntype Type: String\nThe policy type used to contribute to the computation of the query plan.\nlocalDC Type: String\nSpecifies the name of the datacenter closest to Reaper when using the dcAwareRoundRobin policy.\nauthProvider If native protocol authentication is enabled on Cassandra, settings configure Reaper to pass credentials to Cassandra when establishing a connection.\nusername Type: String\nCassandra native protocol username.\npassword Type: String\nCassandra native protocol password.\nH2 or Postgres Database Settings The following settings are specific to a Reaper deployment that is backed by either a H2 or Postgres database. An example of the configuration settings for a Postgres database are as follows.\npostgres: url: jdbc:postgresql://127.0.0.1/reaper user: postgres password: Definitions for the above sub-settings are as follows.\nh2 Settings to configure Reaper to use H2 for storage of its control data.\npostgres Settings to configure Reaper to use Postgres for storage of its control data.\ndriverClass Type: String\nWARNING this setting is DEPRECATED and its usage should be avoided.\nSpecifies the driver to use to connect to the database.\nurl Type: String\nSpecifies the URL to connect to the database (either H2 or Postgres) on.\nuser Type: String\nDatabase username.\npassword Type: String\nDatabase password.\n"
},
{
	"uri": "http://cassandra-reaper.io/backends/",
	"title": "Backends",
	"tags": [],
	"description": "",
	"content": "Cassandra Reaper can be used with either an ephemeral memory storage or persistent database. For persistent scalable database storage, a Cassandra cluster can be set up to back Reaper. To use a Cassandra cluster as the backed storage for Reaper set storageType to a value of cassandra in the Reaper configuration file. Alternatively, a relational database storage; either H2 or Postgres can be set up to back Reaper. To use one of the relational database options as the backed storage for Reaper set storageType to a value of either h2 or postrges in the Reaper configuration file.\nFurther information on the available storage options is provided in the following section.\n In-Memory Cassanda PostgresQL H2  Sample YAML files are available in the src/packaging/resource directory for each of the above storage options:\n cassandra-reaper-memory.yaml cassandra-reaper-cassandra.yaml cassandra-reaper-postgres.yaml cassandra-reaper-h2.yaml  For configuring other aspects of the service, see the available configuration options in the Configuration Reference.\n"
},
{
	"uri": "http://cassandra-reaper.io/usage/health/",
	"title": "Checking a Cluster&#39;s Health",
	"tags": [],
	"description": "",
	"content": "Dashboard When a cluster has been added to Reaper it will be displayed in the dashboard.\n      Node View Clicking on one of the nodes will open a dialog box containing details of the node\u0026rsquo;s state.\n      "
},
{
	"uri": "http://cassandra-reaper.io/configuration/docker_vars/",
	"title": "Docker Variables",
	"tags": [],
	"description": "",
	"content": "The Reaper Docker container has been designed to be highly configurable. Many of the environment variables map directly or indirectly to a settings in the cassandra-reaper.yaml configuration file.\nDirect Mapping to Reaper Specific Configuration Settings The Docker environment variables listed in this section map directly to Reaper specific settings in the cassandra-reaper.yaml configuration file. The following table below lists the Docker environment variables, their associated Reaper specific setting in the cassandra-reaper.yaml configuration file, and the default value assigned by the Docker container (if any). Definitions for each Docker environment variable can be found via the link to the associated setting.\nNote:\nSome variable names have changed between the release of Docker-support and Reaper for Apache Cassandra 1.0. The following Reaper specific variable name changes have occurred in an effort to match closely with the YAML parameter names:\nAssociated Reaper Specific Configuration Settings The following Docker environment variables have no direct mapping to a setting in the cassandra-reaper.yaml configuration file. However, they do affect the content contained in the file that is Reaper specific.\nREAPER_METRICS_ENABLED Type: Boolean\nDefault: false\nAllows the sending of Reaper metrics to a metrics reporting system such as Graphite. If enabled, the other associated environment variables REAPER_METRICS_FREQUENCY and REAPER_METRICS_REPORTERS must be set to appropriate values in order for metrics reporting to function correctly.\nMetrics reporter definition syntax REAPER_METRICS_REPORTERS Type: List\nDefault: []\nDefines the metrics reporters, using a JSON syntax instead of a YAML one. To activate graphite metrics reporting, run the container with the following sample arguments :\ndocker run \\ -p 8080:8080 \\ -p 8081:8081 \\ -e \u0026quot;REAPER_METRICS_ENABLED=true\u0026quot; \\ -e \u0026quot;REAPER_METRICS_FREQUENCY=10 second\u0026quot; \\ -e \u0026quot;REAPER_METRICS_REPORTERS=[{type: graphite, host: my.graphite.host.com, port: 2003, prefix: my.prefix}]\u0026quot; \\ thelastpickle/cassandra-reaper:master Direct Mapping to Cassandra Backend Specific Configuration Settings The Docker environment variables listed in this section map directly to Cassandra backend specific settings in the cassandra-reaper.yaml configuration file. The following table below lists the Docker environment variables, their associated Cassandra backend specific setting in the cassandra-reaper.yaml configuration file, and the default value assigned by the Docker container (if any). Definitions for each Docker environment variable can be found via the link to the associated setting.\nIn order for the Cassandra backend to be used, REAPER_STORAGE_TYPE must be set to cassandra.\nNote:\nSome variable names and defaults have changed between the release of Docker-support and Reaper for Apache Cassandra 1.0. The following Cassandra Backend specific variable name changes have occurred in an effort to match closely with our YAML parameter names:\nThe following default values have changed:\nAssociated Cassandra Backend Specific Configuration Settings The following Docker environment variables have no direct mapping to a setting in the cassandra-reaper.yaml configuration file. However, the do affect the content contained in the file that is Cassandra backend specific.\nREAPER_CASS_AUTH_ENABLED Type: Boolean\nDefault: false\nAllows Reaper to send authentication credentials when establishing a connection with Cassandra via the native protocol. When enabled, authentication credentials must be specified by setting values for REAPER_CASS_AUTH_USERNAME and REAPER_CASS_AUTH_PASSWORD.\nREAPER_CASS_NATIVE_PROTOCOL_SSL_ENCRYPTION_ENABLED Type: Boolean\nDefault: false\nAllows Reaper to establish an encrypted connection when establishing a connection with Cassandra via the native protocol.\nDirect Mapping to H2 or Postgres Backend Configuration Settings The Docker environment variables listed in this section map directly to H2/Postgres backend specific settings in the cassandra-reaper.yaml configuration file. The following table below lists the Docker environment variables, their associated H2/Postgres backend specific setting in the cassandra-reaper.yaml configuration file, and the default value assigned by the Docker container (if any). Definitions for each Docker environment variable can be found via the link to the associated setting.\nIn order to use the following settings, REAPER_STORAGE_TYPE must be set to h2 or postgres.\nNote:\nSome variable names have changed between the release of Docker-support and Reaper for Apache Cassandra 1.0. The following Reaper specific variable name changes have occurred in an effort to match closely with the YAML parameter names:\n"
},
{
	"uri": "http://cassandra-reaper.io/install/upgrade/",
	"title": "Upgrade Guide",
	"tags": [],
	"description": "",
	"content": "Upgrading from 1.2.0/1.2.1 to 1.2.2 We unfortunately had to break schema migrations for upgrades to 1.2.2 from 1.2.0 and 1.2.1. Here is the upgrade procedure for each Reaper backend:\nCassandra  Stop all Reaper instances Run the following DDL statement on the backend cluster :  ALTER TABLE reaper_db.repair_unit_v1 DROP repair_thread_count;  Upgrade Reaper to 1.2.2 and start it  Note: by doing so the number of threads for each existing schedule will revert back to 0, which will be translated to a single repair thread. If you previously had defined a specific number of threads you will need to recreate these schedules or update the repair_unit_v1 table manually (this field is only useful with Cassandra 2.2, there is no benefit in Cassandra \u0026lt;=2.1 or \u0026gt;=3.0).\nH2  Stop Reaper Pointing to your current Reaper jar file (located in /usr/share/cassandra-reaper for packaged installs), run the following command :  java -cp /usr/share/cassandra-reaper/cassandra-reaper-1.2.*.jar org.h2.tools.Shell When asked, provide the JDBC URL from your Cassandra Reaper yaml file (for example : jdbc:h2:~/reaper-db/db;MODE=PostgreSQL), use the default Driver and leave username/password empty. Then run the following statements:\nALTER TABLE REPAIR_UNIT DROP COLUMN repair_thread_count; ALTER TABLE REPAIR_SEGMENT DROP COLUMN TOKEN_RANGES;  Upgrade Reaper to 1.2.2 and start it  Note: by doing so the number of threads for each existing schedule will revert back to 0, which will be translated to a single repair thread. If you previously had defined a specific number of threads you will need to recreate these schedules or update the repair_unit table manually (this field is only useful with Cassandra 2.2, there is no benefit in Cassandra \u0026lt;=2.1 or \u0026gt;=3.0).\nPostgres  Stop Reaper Connect to your Postgres database using the psql shell (or any other client) and run the following statements:  ALTER TABLE \u0026quot;repair_unit\u0026quot; DROP COLUMN \u0026quot;repair_thread_count\u0026quot;; ALTER TABLE \u0026quot;repair_segment\u0026quot; DROP COLUMN \u0026quot;token_ranges\u0026quot;;  Upgrade Reaper to 1.2.2 and start it  Note: by doing so the number of threads for each existing schedule will revert back to 0, which will be translated to a single repair thread. If you previously had defined a specific number of threads you will need to recreate these schedules or update the repair_unit table manually (this field is only useful with Cassandra 2.2, there is no benefit in Cassandra \u0026lt;=2.1 or \u0026gt;=3.0).\nUpgrading from 1.2.2 to 2.0.0 TODO: Add information about upgrading from 1.2.2 to 2.0.0\n"
},
{
	"uri": "http://cassandra-reaper.io/configuration/",
	"title": "Configuration Reference",
	"tags": [],
	"description": "",
	"content": "An example testing configuration YAML file can be found from within this project repository: src/server/src/test/resources/cassandra-reaper.yaml.\nThe configuration file structure is provided by Dropwizard, and help on configuring the server, database connection, or logging, can be found on the Dropwizard Configuration Reference\nThe configuration is broken into the following sections:\n Reaper Specific - Provides details on settings specific to Reaper. Backend Specific - Provides details on settings specific to the different backends that can be used with Reaper; Cassandra, H2 and Postgres. Docker Variables - Provides details on the Docker Variables that can be used to configure Reaper.  Note that Cassandra backend configuration relies on the Dropwizard-Cassandra module.\n"
},
{
	"uri": "http://cassandra-reaper.io/install/building/",
	"title": "Build from Source Guide",
	"tags": [],
	"description": "",
	"content": "Building Install Packages Debian packages and RPMs can be built from this project using Make, for example:\nmake deb make rpm Building JARs from source To build and run tests use the following command:\nmvn clean package You can skip the tests if you just want to build using the following command:\nmvn clean package -DskipTests Building Docker Image from source See the Docker section for more details.\nBuilding Using Docker To simplify the build toolchain it\u0026rsquo;s possible to build everything using Docker itself. This is the process used to build the release binary artifacts from jar files to debian packages.\nBuilding Reaper packages requires quite a few dependencies, especially when making changes to the web interface code. In an effort to simplify the build process, Dockerfiles have been created that implement the build actions required to package Reaper.\nTo build the JAR and other packages which are then placed in the src/packages directory run the following commands:\ncd src/packaging/docker-build docker-compose build docker-compose run build "
},
{
	"uri": "http://cassandra-reaper.io/metrics/",
	"title": "Metrics",
	"tags": [],
	"description": "",
	"content": "Metrics Reporting Reaper metrics are provided via the Dropwizard Metrics interface. The interface gives Reaper the ability to configure various metrics reporting systems. Metrics reporting can be configured in Reaper with the metrics property in the Reaper configuration YAML file. The metrics property has two fields; frequency and reporters. Specific metric reporters are defined in the reporters field as follows.\nmetrics: frequency: 1 minute reporters: - type: \u0026lt;type\u0026gt; "
},
{
	"uri": "http://cassandra-reaper.io/usage/",
	"title": "Using Reaper",
	"tags": [],
	"description": "",
	"content": "This section discusses the normal usage of Reaper on a day to day basis.\nReaper includes a community-driven web interface that can be accessed at:\nhttp://$REAPER_HOST:8080/webui/index.html\nThe web interface provides the ability to:\n Add/remove clusters Manage repair schedules Run manual repairs and manage running repairs  "
},
{
	"uri": "http://cassandra-reaper.io/usage/single/",
	"title": "Running a Cluster Repair",
	"tags": [],
	"description": "",
	"content": "Reaper has the ability to launch a once-off repair on a cluster. This can be done in the following way.\nStart a New Repair Click the repair menu item on the left side to navigate to the Repair page. Click Start a new repair to open the repair details form.\n      Fill in the Details Enter values for the keyspace, tables, owner and other fields and click the Repair button. See the table below for further information on the details for each field.\n      "
},
{
	"uri": "http://cassandra-reaper.io/usage/schedule/",
	"title": "Scheduling a Cluster Repair",
	"tags": [],
	"description": "",
	"content": "Reaper has the ability to create and manage repair schedules for a cluster. This can be done in the following way.\nSetup a Repair Schedule Click the schedule menu item on the left side to navigate to the Schedules page. Click Add schedule to open the schedule details form.\n      Fill in the Details Enter values for the keyspace, tables, owner and other fields and click Add Schedule button. The details for adding a schedule are similar to the details for Repair form except the \u0026ldquo;Clause\u0026rdquo; field is replaced with two fields; \u0026ldquo;Start time\u0026rdquo; and \u0026ldquo;Interval in days\u0026rdquo;. See the table below for further information the two fields.\n      After creating a scheduled repair, the page is updated with a list of active and paused repair schedules.\nNote that when choosing to add a new repair schedule, it is recommended to restrict the repair schedules to specific tables, instead of scheduling repairs for an entire keyspace. Creating different repair schedules will allow for simpler scheduling, fine-grain tuning for more valuable data, and easily grouping tables with smaller data load into different repair cycles.\nFor example, if there are certain tables that contain valuable data or a business requirement for high consistency and high availability, they could be schedule to be repaired during low traffic periods.\nNote that scheduled repairs can be paused and deleted by users with access to the Reaper web interface. To add authentication security the web UI see the authentication section for further information.\n"
},
{
	"uri": "http://cassandra-reaper.io/usage/multi_dc_non-distributed/",
	"title": "Operating Multiple DCs with a Single Reaper",
	"tags": [],
	"description": "",
	"content": "Reaper can operate clusters which has a multi datacenter deployment. The datacenterAvailability setting in the Reaper YAML file indicates to Reaper its deployment in relation to cluster data center network locality.\nSingle Reaper instance with JMX accessible for all DCs In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in all DCs, it is possible to have a single instance of Reaper handle one or multiple clusters by using the following setting in the configuration yaml file :\ndatacenterAvailability: ALL This setup works with all backends : Apache Cassandra, Memory, H2 and Postgres.\n      Reaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nThe keyspaces must be replicated using NetworkTopologyStrategy (NTS) and have replicas at least on the DC Reaper can access through JMX. Repairing the remote DC will be handled internally by Cassandra.\nNote : multiple instances of Reaper can be running at once with this setting only when using the Apache Cassandra backend. See distributed mode for more details.\nSingle Reaper instance with JMX accessible for limited DCs In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in only some of the DCs, it is possible to have a single instance of Reaper handle one or multiple clusters by using the following setting in the configuration yaml file :\ndatacenterAvailability: LOCAL Be aware that this setup will not allow to handle backpressure for those remote DCs as JMX metrics (pending compactions, running repairs) from those remote nodes are not made available to Reaper.\nIf multiple clusters are registered in Reaper it is required that Reaper can access all nodes in at least one data center in each of the registered clusters.\nThis setup works with all backends : Apache Cassandra, Memory, H2 and Postgres.\n      Reaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nThe keyspaces must be replicated using NetworkTopologyStrategy (NTS) and have replicas at least on the DC Reaper can access through JMX. Repairing the remote DC will be handled internally by Cassandra.\nNote : multiple instances of Reaper can be running at once with this settings only using when the Apache Cassandra backend. See distributed mode for more details.\n"
},
{
	"uri": "http://cassandra-reaper.io/usage/multi_dc_distributed/",
	"title": "perating Multiple DCs using Multiple Reaper",
	"tags": [],
	"description": "",
	"content": "Multiple Reaper instances can operate clusters which have multi datacenter deployment. Multiple Reaper instances, also known as Distributed mode, can only be used when using the Apache Cassandra backend. Using multiple Reaper instances allows improved availability and fault tolerance. It is more likely that a Reaper UI is available via one of the Reaper instances, and that scheduled repairs are executed by one of the running Reaper instances.\nThe datacenterAvailability setting in the Reaper YAML file indicates to Reaper its deployment in relation to cluster data center network locality.\nMultiple Reaper instances with JMX accessible for all DCs In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in all DCs, it is possible to have multiple instances of Reaper handle one or multiple clusters by using the following setting in the configuration yaml file :\ndatacenterAvailability: ALL       Reaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nMultiple Reaper instances with JMX accessible for limited DCs In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in only some of the DCs, it is possible to have multiple instances of Reaper handle one or multiple clusters by using the following setting in the configuration yaml file :\ndatacenterAvailability: LOCAL Note, there is no backpressure for nodes in any datacenter if no Reaper instances have JMX access to that datacenter. This is because the JMX metrics (pending compactions, running repairs) required for backpressure is not available from those remote nodes.\nIf multiple clusters are registered in Reaper it is required that some Reaper instances can access all the nodes in at least one of the datacenters in each of the registered clusters.\nLOCAL mode allows you to register multiple clusters in a distributed Reaper installation. LOCAL mode also allows you to prioritize repairs running according to their schedules over worrying about the load on remote and unaccessible datacenters and nodes.\n      Reaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nAny keyspaces that only have replicas in remote JMX unreachable datacenters can not be repaired by Reaper.\nMultiple Reaper instances with JMX accessible locally to each DC In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in the current DC only, it is possible to have a multiple instances of Reaper running in different DCs by using the following setting in the configuration yaml file:\ndatacenterAvailability: EACH This setup prioritises handling backpressure on all nodes over running repairs. Where latency of requests and availability of nodes takes precedence over scheduled repairs this is the safest setup in Reaper.\nThere must be installed and running a Reaper instance in every datacenter of every registered Cassandra cluster. And every Reaper instance must have CQL access to the backend Cassandra cluster it uses as a backend.\n      Reaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nMultiple Reaper instances with JMX access restricted to localhost By default, Cassandra starts up with JMX access restricted to the local machine. This is considered by many companies as being a security requirement. In this case, one reaper instance must be collocated as a sidecar with each Cassandra process, using the following setting in the configuration yaml file:\ndatacenterAvailability: SIDECAR There must be installed and running a Reaper instance on each Cassandra node in the cluster. And every Reaper instance must have CQL access to the backend Cassandra cluster it uses as a backend.\n      Reaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nMore informations on the sidecar mode can be found on this page.\n"
},
{
	"uri": "http://cassandra-reaper.io/usage/sidecar_mode/",
	"title": "Sidecar Mode",
	"tags": [],
	"description": "",
	"content": "Sidecar Mode is a way of deploying Cassandra Reaper with one reaper instance for each node in the cluster. The name \u0026ldquo;Sidecar\u0026rdquo; comes from the Sidecar Pattern which describes a mechanism for co-locating an auxiliary service with its supported application. See also Design Patterns for Container-based Distributed Systems. It is a pattern that is often used in Kubernetes, where the main application and the sidecar application are deployed as separate containers in the same pod.\nIn Sidecar Mode, each Cassandra node process is deployed alongside a Reaper process; Cassandra is the parent application and Reaper is the sidecar.\nAdvantages  Security: Each Reaper process only needs local JMX access. No need to configure remote JMX access and JMX authentication. Kubernetes Friendly: Sidecar mode is very easy to setup if your Cassandra Cluster is deployed using a container orchestration system, such as Kubernetes.  Guidance  Deploy one Reaper cluster per Cassandra cluster: Sidecar mode has been designed to allow you to easily deploy a separate, highly available Reaper service for each of your Cassandra clusters. You currently cannot use a Sidecar Mode Reaper service to manage multiple clusters. Deploy Reaper in a sidecar container alongside Cassandra: If you are using Kubernetes to deploy Cassandra and Reaper, put the Reaper process in the same Pod as Cassandra, in a sidecar container. See Pods that run multiple containers that need to work together Use Spreaper and the REST API for Reaper administration (use the WebUI for reporting): If you have automated the deployment of Reaper in Sidecar Mode alongside a Cassandra Cluster (described above). Clusters in sidecar mode are self registered into Reaper, and you can turn on auto-scheduling to fully automate Reaper setup. Use or write an operator: For example, you could write a Kubernetes Custom Resource Definition (e.g. a Cassandra resource) which contains configuration fields for the Cassandra database and fields with which you can define the Reaper repair settings. Then write an accompanying Kubernetes Operator which watches for changes to those resources and reconciles the declared (desired) configuration with the actual state of the Cassandra cluster and its Reaper service.  Caveats  One Reaper cluster per Cassandra Cluster: Sidecar Mode is designed to be used in situations where you deploy a separate Reaper cluster for every Cassandra cluster. If you prefer to deploy a single Reaper cluster to manage multiple Cassandra clusters then Sidecar Mode may not be suitable for you. Container orchestration (Kubernetes) preferred: Following on from the \u0026ldquo;one-reaper-per-cluster\u0026rdquo; caveat (above) it is also worth noting that it is easiest to deploy Reaper in Sidecar Mode if your Cassandra cluster is managed by a container orchestration system, such as Kubernetes. Kubernetes ensures that the Reaper process starts and dies with the Cassandra process. Kubernetes also ensures that the Reaper process shares a network namespace with the Cassandra process so that it (and only it) can access the JMX service. High Resource Usage: In Sidecar Mode you will be deploying an additional Java process alongside each Cassandra process. If your underlying infrastructure has limited resources, then Sidecar Mode may not be suitable for you. WebUI Session Affinity: In Sidecar Mode, you will be able to connect to the Web based administration pages of any Reaper process in the cluster. But even once you have logged into the Web UI of one Reaper process, you will not be able to log into the Web UI on other Reaper processes. The Web UI session information is not stored in the underlying Cassandra Backend database. You will need to setup a loadbalancer / service that ensures that all Web UI traffic from a particular client is directed to a particular Reaper process. Snapshot support: In Sidecar Mode, snapshot are not supported as Reaper currently tries to connect to all nodes directly for that feature. Asynchronous orchestration of such tasks is planned for a future release.  "
},
{
	"uri": "http://cassandra-reaper.io/configuration/authentication/",
	"title": "Authentication",
	"tags": [],
	"description": "",
	"content": "Authentication is activated in Reaper by default. It relies on Apache Shiro, which allows to store users and password in files, databases or connect through LDAP and Active Directory out of the box. The default authentication uses the dummy username and password as found in the default shiro.ini. It is expected you override this in a production environment.\nThis default Shiro authentication configuration is referenced via the following block in the Reaper yaml file :\naccessControl: sessionTimeout: PT10M shiro: iniConfigs: [\u0026#34;classpath:shiro.ini\u0026#34;] Default settings As of Reaper 1.4.0, authentication is enabled by default and credentials are:\n Username: admin Password: admin  With clear passwords Copy the default shiro.ini file and adapt it overriding the \u0026ldquo;users\u0026rdquo; section :\n… [users] user1 = password1 user2 = password2 … With encrypted passwords Based on Shiro\u0026rsquo;s document on Encrypting passwords :\n[main] authc = org.apache.shiro.web.filter.authc.PassThruAuthenticationFilter authc.loginUrl = /webui/login.html sha256Matcher = org.apache.shiro.authc.credential.Sha256CredentialsMatcher iniRealm.credentialsMatcher = $sha256Matcher [users] john = 807A09440428C0A8AEF58BD3ECE32938B0D76E638119E47619756F5C2C20FF3A … To generate a password, you case use for example :\n From the command line :  echo -n \u0026#34;Hello World\u0026#34; | shasum -a 256 echo -n \u0026#34;Hello World\u0026#34; | sha256sum  Or some language of your choice (like Python here) :  import hashlib hash_object = hashlib.sha256(b\u0026#39;Hello World\u0026#39;) hex_dig = hash_object.hexdigest() print(hex_dig) a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e With LDAP accounts Based on Shiro\u0026rsquo;s LDAP realm usage.\nAn example configuration for LDAP authentication exists (commented out) in the default shiro.ini :\n # Example LDAP realm, see https://shiro.apache.org/static/1.2.4/apidocs/org/apache/shiro/realm/ldap/JndiLdapContextFactory.html ldapRealm = org.apache.shiro.realm.ldap.JndiLdapRealm ldapRealm.userDnTemplate = uid={0},ou=users,dc=cassandra-reaper,dc=io ldapRealm.contextFactory.url = ldap://ldapHost:389 ;ldapRealm.contextFactory.authenticationMechanism = DIGEST-MD5 ;ldapRealm.contextFactory.systemUsername = cn=Manager, dc=example, dc=com ;ldapRealm.contextFactory.systemPassword = secret ;ldapRealm.contextFactory.environment[java.naming.security.credentials] = ldap_password Accessing Reaper via the command line spreaper In order to interact with Reaper through spreaper when Shiro authentication is activated, you will first need to login as follows:\n$ ./spreaper login admin Password: ***** # Logging in... You are now authenticated to Reaper. # JWT saved The JWT will be saved in ~/.reaper/jwt and automatically used by Reaper for any other call. Pre-existing tokens can be passed to spreaper using --jwt \u0026lt;token\u0026gt; with any call.\nAccessing Reaper via the REST API The RESTful endpoints to Reaper can also be authenticated using JWT (Java Web Token).\nA token can be generated from the /jwt url by passing the Session ID cookie retrieved from the /login endpoint. Logging in through /login is described in the REST Api page.\n"
},
{
	"uri": "http://cassandra-reaper.io/development/api/",
	"title": "REST API",
	"tags": [],
	"description": "",
	"content": "Source code for all the REST resources can be found from package io.cassandrareaper.resources.\nLogin Resource  POST /login  Expected form parameters:  username: User to login with as defined in Shiro settings (default user is admin) password: Password to authenticate with through Shiro (default password of user admin is: admin) rememberMe: Boolean to have the Web UI remember the username   Endpoint for logging in to Reaper    Shiro JWT Provider  GET /jwt  Expected query parameters: None Returns a JWT to use in all REST calls when authentication is turned on in Reaper. The token must be passed in the Authorization HTTP header in the following form:    Authorization: Bearer [JWT value] This operation expects that a call was previously made to /login and that the retrieved session id is passed in the cookies when requesting a JWT.\nPing Resource  GET /ping  Expected query parameters: None Simple ping resource that can be used to check whether the reaper is running.    Cluster Resource   GET /cluster\n Expected query parameters:  seedHost: Limit the returned cluster list based on the given seed host. (Optional)   Returns a list of registered cluster names in the service.    GET /cluster/{cluster_name}\n Expected query parameters:  limit: Limit the number of repair runs returned. Recent runs are prioritized. (Optional)   Returns a cluster object identified by the given \u0026ldquo;cluster_name\u0026rdquo; path parameter.    GET /cluster/{cluster_name}/tables\n Expected query parameters: None Returns a map of \u0026lt;KeyspaceName, List\u0026lt;TableName\u0026gt;\u0026gt;    POST /cluster\n Expected query parameters:  seedHost: Host name or IP address of the added Cassandra clusters seed host.   Adds a new cluster to the service, and returns the newly added cluster object, if the operation was successful. If the cluster is already registered, the list of host will be updated to match the current topology.    PUT /cluster/{cluster_name}\n Expected query parameters:  seedHost: New host name or IP address used as Cassandra cluster seed.   Modifies a cluster\u0026rsquo;s seed host. Comes in handy when the previous seed has left the cluster.    DELETE /cluster/{cluster_name}\n Expected query parameters:  force : Enforce deletion of the cluster even if there are active schedules and a repair run history (Optional)   Delete a cluster object identified by the given \u0026ldquo;cluster_name\u0026rdquo; path parameter. Cluster will get deleted only if there are no schedules or repair runs for the cluster, or the request will fail. Delete repair runs and schedules first before calling this.    Repair Run Resource   GET /repair_run\n Optional query parameters: * state: Comma separated list of repair run state names. Only names found in io.cassandrareaper.core.RunState are accepted. Returns a list of repair runs, optionally fetching only the ones with state state.    GET /repair_run/{id}\n Expected query parameters: None Returns a repair run object identified by the given \u0026ldquo;id\u0026rdquo; path parameter.    GET /repair_run/cluster/{cluster_name}\n Expected query parameters: None Returns a list of all repair run statuses found for the given \u0026ldquo;cluster_name\u0026rdquo; path parameter.    GET /repair_run/{id}/segments\n Expected query parameters: None Returns the list of segments of the repair run.    POST /repair_run/{id}/segments/abort/{segment_id}\n Expected query parameters: None Aborts a running segment and puts it back in NOT_STARTED state. The segment will be processed again later during the lifetime of the repair run.    POST /repair_run\n Expected query parameters: * clusterName: Name of the Cassandra cluster. * keyspace: The name of the table keyspace. * tables: The name of the targeted tables (column families) as comma separated list. If no tables given, then the whole keyspace is targeted. (Optional) * owner: Owner name for the run. This could be any string identifying the owner. * cause: Identifies the process, or cause the repair was started. (Optional) * segmentCount: Defines the amount of segments per node to create for the repair run. (Optional) * repairParallelism: Defines the used repair parallelism for repair run. (Optional) * intensity: Defines the repair intensity for repair run. (Optional) * incrementalRepair: Defines if incremental repair should be done. [true/false] (Optional) * nodes : a specific list of nodes whose tokens should be repaired. (Optional) * datacenters : a specific list of datacenters to repair. (Optional) * blacklistedTables : The name of the tables that should not be repaired. Cannot be used in conjunction with the tables parameter. (Optional) * repairThreadCount : Since Cassandra 2.2, repairs can be performed with up to 4 threads in order to parallelize the work on different token ranges. (Optional)  Endpoint used to create a repair run. Does not allow triggering the run. Creating a repair run includes generating the repair segments. Notice that query parameter \u0026ldquo;tables\u0026rdquo; can be a single String, or a comma-separated list of table names. If the \u0026ldquo;tables\u0026rdquo; parameter is omitted, and only the keyspace is defined, then created repair run will target all the tables in the keyspace. Returns the ID of the newly created repair run if successful.      PUT /repair_run/{id}/state/{state}\n Expected query parameters: None Starts, pauses, or resumes a repair run identified by the \u0026ldquo;id\u0026rdquo; path parameter.\nCan also be used to reattempt a repair run in state \u0026ldquo;ERROR\u0026rdquo;, picking up where it left off.\nPossible values for given state are: \u0026ldquo;PAUSED\u0026rdquo; or \u0026ldquo;RUNNING\u0026rdquo;.    PUT /repair_run/{id}/intensity/{intensity}\n Expected query parameters: * intensityStr : New value for the intensity of the repair run. Expected is a float between 0.0 and 1.0. Modifies the intensity of a PAUSED repair run. Returns OK if all goes well NOT_MODIFIED if new state is the same as the old one, and 409 (CONFLICT) if transition is not supported.    DELETE /repair_run/{id}\n Expected query parameters:  owner: Owner name for the run. If the given owner does not match the stored owner, the delete request will fail.   Delete a repair run object identified by the given \u0026ldquo;id\u0026rdquo; path parameter. Repair run and all the related repair segments will be deleted from the database.    Repair Schedule Resource   GET /repair_schedule\n Expected query parameters:  clusterName: Filter the returned schedule list based on the given cluster name. (Optional) keyspaceName: Filter the returned schedule list based on the given keyspace name. (Optional)   Returns all repair schedules present in the Reaper    GET /repair_schedule/{id}\n Expected query parameters: None Returns a repair schedule object identified by the given \u0026ldquo;id\u0026rdquo; path parameter.    GET /repair_schedule/cluster/{cluster_name}\n Expected query parameters: None Returns the repair schedule objects for the given cluster.    POST /repair_schedule\n Expected query parameters: * clusterName: Name of the Cassandra cluster. * keyspace: The name of the table keyspace. * tables: The name of the targeted tables (column families) as comma separated list. If no tables given, then the whole keyspace is targeted. (Optional) * owner: Owner name for the schedule. This could be any string identifying the owner. * segmentCount: Defines the amount of segments to create for scheduled repair runs. (Optional) * repairParallelism: Defines the used repair parallelism for scheduled repair runs. (Optional) * intensity: Defines the repair intensity for scheduled repair runs. (Optional) * incrementalRepair: Defines if incremental repair should be done. [true/false] (Optional) * scheduleDaysBetween: Defines the amount of days to wait between scheduling new repairs. For example, use value 7 for weekly schedule, and 0 for continuous. * scheduleTriggerTime: Defines the time for first scheduled trigger for the run. If you don\u0026rsquo;t give this value, it will be next mid-night (UTC). Give date values in ISO format, e.g. \u0026ldquo;2015-02-11T01:00:00\u0026rdquo;. (Optional) * nodes : a specific list of nodes whose tokens should be repaired. (Optional) * datacenters : a specific list of datacenters to repair. (Optional) * blacklistedTables : The name of the tables that should not be repaired. Cannot be used in conjunction with the tables parameter. (Optional) * repairThreadCount : Since Cassandra 2.2, repairs can be performed with up to 4 threads in order to parallelize the work on different token ranges. (Optional)  Create and activate a scheduled repair.      DELETE /repair_schedule/{id}\n Expected query parameters:  owner: Owner name for the schedule. If the given owner does not match the stored owner, the delete request will fail.   Delete a repair schedule object identified by the given \u0026ldquo;id\u0026rdquo; path parameter. Repair schedule will get deleted only if there are no associated repair runs for the schedule. Delete all the related repair runs before calling this endpoint.    PUT /repair_schedule/{id}\n Expected query parameters: None Returns the repair schedule objects for the given cluster.    Snapshot Resource   GET /snapshot/{clusterName}/{host}\n Expected query parameters: None Lists snapshots for the given host in the given cluster.    GET /snapshot/cluster/{clusterName}\n Expected query parameters: None Lists all snapshots for the the given cluster.    DELETE /snapshot/{clusterName}/{host}/{snapshotName}\n Expected query parameters: None Deletes a specific snapshot on a given node.    DELETE /snapshot/{clusterName}/{snapshotName}\n Expected query parameters: None Deletes a specific snapshot on all nodes in a given cluster.    POST /snapshot/{clusterName}\n Expected query parameters: * keyspace: Name of the Cassandra cluster. * tables: The name of the targeted tables (column families) as comma separated list. If no tables given, then the whole keyspace is targeted. (Optional) * snapshot_name: name to use for the snapshot. (Optional) * owner: Owner name for the run. This could be any string identifying the owner. * cause: Identifies the process, or cause the repair was started. (Optional)  Create a snapshot on all hosts in a cluster, using the same name.      POST /snapshot/{clusterName}/{host}\n Expected query parameters: * keyspace: Name of the Cassandra cluster. * tables: The name of the targeted tables (column families) as comma separated list. If no tables given, then the whole keyspace is targeted. (Optional) * snapshot_name: name to use for the snapshot. (Optional)  Create a snapshot on a specific host.      "
},
{
	"uri": "http://cassandra-reaper.io/usage/cassandra-diagnostics/",
	"title": "Cassandra Diagnostic Events",
	"tags": [],
	"description": "",
	"content": "Reaper has the ability to listen and display live Cassandra\u0026rsquo;s emitted Diagnostic Events.\nIn Cassandra 4.0 internal system \u0026ldquo;diagnostic events\u0026rdquo; have become available, via the work done in CASSANDRA-12944. These allow to observe internal Cassandra events, for example in unit tests and with external tools. These diagnostic events provide operational monitoring and troubleshooting beyond logs and metrics.\nEnabling Diagnostic Events server-side in Apache Cassandra 4.0 Available from Apache Cassandra version 4.0, Diagnostic Events are not enabled (published) by default.\nTo enable the publishing of diagnostic events, on the Cassandra node enable the diagnostic_events_enabled flag.\n# Diagnostic Events # # If enabled, diagnostic events can be helpful for troubleshooting operational issues. Emitted events contain details # on internal state and temporal relationships across events, accessible by clients via JMX. diagnostic_events_enabled: true Restarting the node is required after this change.\nReaper In Reaper go to the \u0026ldquo;Live Diagnostics\u0026rdquo; page.\nSelect the cluster that is running Cassandra version 4.0 with diagnostic_events_enabled: true, using the \u0026ldquo;Filter cluster\u0026rdquo; field.\nExpand the \u0026ldquo;Add Events Subscription\u0026rdquo; section.\nType in a description for the subscription to be created. Select the node you want to observe diagnostic events to. Select the diagnostic events you want to observe. Check \u0026ldquo;Enable Live View\u0026rdquo;.\nPress \u0026ldquo;Save\u0026rdquo;. In the list of subscriptions there should now be a row with what was entered.\nPress \u0026ldquo;View\u0026rdquo;. A green bar will appear. Underneath this green bar will appear diagnostic events of the types and from the nodes subscribed to. Press the green bar to stop displaying live events.\n"
},
{
	"uri": "http://cassandra-reaper.io/community/",
	"title": "Community",
	"tags": [],
	"description": "",
	"content": "We have a Mailing List and Gitter chat available.\n"
},
{
	"uri": "http://cassandra-reaper.io/faq/",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "",
	"content": "Why use Reaper instead of noddetool + cron? While it\u0026rsquo;s possible to set up crontab to call nodetool, it requires staggering the crons to ensure overlap is kept to a minimum. Reaper is able to intelligently schedule repairs to avoid putting too much load on the cluster, avoiding impacting performance. Reaper also offers a simple UI to schedule repairs as granularly as needed.\nDo I need to do repairs if I\u0026rsquo;m not deleting data? Yes! Repair is a necessary anti-entropy mechanism that keeps your cluster consistent. Without repair, queries at LOCAL_ONE could return incorrect results.\nWhy are there four backends? Which should i use? When we (The Last Pickle) took over development of Reaper, we found it cumbersome to require a PostGres database in addition to the Cassandra database. We also knew Reaper would need to be fault tolerant and work across datacenters. The most straightforward way to do this would be to leverage Cassandra\u0026rsquo;s fault tolerance.\nFor small setups, using a local DB (H2) can make sense. This would allow reaper to run on a single node using EBS for storage. This is intentionally a very simple design to keep costs down and avoid extra operational work.\nFor new installations, we\u0026rsquo;re recommending the Cassandra backend as it allows the most flexibility, with the highest level of availability.\n"
},
{
	"uri": "http://cassandra-reaper.io/development/",
	"title": "Development",
	"tags": [],
	"description": "",
	"content": "Forward Merging Reaper practices forward merging commits.\nFixes and improvements required to release branches are first committed to those branches. These changes are merged forward onto master afterwards.\nAn example where a bugfix developed and approved for the release branch 1.4 is to be merged;\n# first rebase the work one last time off the latest 1.4 branch git checkout bob/1.4_bugfix git rebase 1.4 git push # switch to the 1.4 branch and fast-forward merge in the fix git checkout 1.4 git merge bob/1.4_bugfix # take note of the sha of the fast-forward merge commit that just happened # switch to master git checkout master # create the empty merge commit git merge 1.4 -s ours # cherry-pick the change using the sha noted above, and make any manual adjustments required against the master branch git cherry-pick -n \u0026lt;sha\u0026gt; # commit amend the forward ported changes into the merge commit git commit -a --amend # push both 1.4 and master branches at the same time git push origin 1.4 master --atomic For more information on the value of forward merging, and the principles of \u0026ldquo;merge down, copy up\u0026rdquo; and the \u0026ldquo;tofu scale: firm above, soft below\u0026rdquo;, see\n https://www.perforce.com/perforce/conferences/us/2005/presentations/Wingerd.pdf https://www.youtube.com/watch?v=AJ-CpGsCpM0  Cutting Releases Cutting a release involves the following steps. You will see these terms referenced by the steps.\n \u0026lt;RELEASE_BRANCH\u0026gt; - The major version of the release branch you will be releasing from. e.g. 3.0. \u0026lt;RELEASE_VERSION\u0026gt; - The version of release you want to create. e.g. 3.0.7.  Where you see the term, substitute it for the value that applies to the release.\n1. Check if the code base is ready for release Check with the community if the codebase is ready for the release. This includes checking any outstanding issues or pull requests in progress.\n2. Change to the major release branch If you are cutting a new major release, you will need to create a new major release branch. Otherwise, you can simply change to the major release branch you want to release from.\nCreate a new major release branch Run the following commands only if you are cutting a new major release. In this case, you will need to create a release branch off the master branch. Release branches follow the format: 1.0, 1.1, 1.2, 1.3, etc.\ngit checkout master git pull --rebase --prune git checkout -b \u0026lt;RELEASE_BRANCH\u0026gt; git push origin \u0026lt;RELEASE_BRANCH\u0026gt; Change to the major release branch Run the following command only if you are cutting a release from an existing major release branch. In this case, all you need to do is change to the major release branch.\ngit checkout \u0026lt;RELEASE_BRANCH\u0026gt; 3. Check the build status Check the build for the major release branch you are creating the release from. It must have a green status before a release can be made. You can check this in GitHub actions.\n      4. Update the change log Generate the changes to append to CHANGELOG.md. You will need to do this using github-changes. If this is your first time cutting a release you will probably need to install it. The install instructions are on its GitHub landing page.\nRun the following github-changes command to get the list of changes for the release branch.\ngithub-changes \\ -o thelastpickle \\ -r cassandra-reaper \\ --use-commit-body \\ -f changelog.tmp \\ -k \u0026lt;GITHUB_PERSONAL_ACCESS_TOKEN\u0026gt; \\ -b \u0026lt;COMMIT_SHA\u0026gt; \\ -v Where:\n \u0026lt;GITHUB_PERSONAL_ACCESS_TOKEN\u0026gt; - Your GitHub Personal Access Token. There is documentation on GitHub help if you need to create one. \u0026lt;COMMIT_SHA\u0026gt; - The SHA of the latest commit on the branch you want to release.  The above command will write the changes for the release branch to a file named changelog.tmp. Use the following commands to extract the most recent set of changes that will go into this release. Remember to substitute \u0026lt;RELEASE_VERSION\u0026gt; on the first line below with the release version number.\nRELEASE_VERSION=\u0026lt;RELEASE_VERSION\u0026gt; BLOCK_START=$(grep -n -m 1 \u0026quot;###\u0026quot; changelog.tmp \\ | cut -d':' -f1) BLOCK_END=$(grep -n -m 2 \u0026quot;###\u0026quot; changelog.tmp \\ | cut -d':' -f1 | tail -n 1) head -n $((${BLOCK_END}-1)) changelog.tmp \\ | tail -n $((${BLOCK_END}-${BLOCK_START})) \\ | sed \u0026quot;s/^###\\ upcoming/###\\ ${RELEASE_VERSION}/g\u0026quot; Copy the entries generated by the previous command and paste them in the CHANGELOG.md file on the release branch. Specifically, paste the entries in between the ## Change Log and the first release heading denoted by the ###. Commit the updated CHANGELOG.md to the branch with the specified comment and push to the remote repository.\ngit add CHANGELOG.md git commit -m \u0026quot;Updated changelog for \u0026lt;RELEASE_VERSION\u0026gt;\u0026quot; git push origin \u0026lt;RELEASE_BRANCH\u0026gt; 5. Change the project version number The project version number makes up the name of a binary generated for the release. You will need to update it using Maven.\nmvn -B versions:set \u0026quot;-DnewVersion=\u0026lt;RELEASE_VERSION\u0026gt;\u0026quot; Commit the changes to the branch with the specified comment and push to the remote repository.\ngit add pom.xml git add src/server/pom.xml git commit -m \u0026quot;Release \u0026lt;RELEASE_VERSION\u0026gt;\u0026quot; git push origin \u0026lt;RELEASE_BRANCH\u0026gt; 6. Create the release tag Tag the release branch with the release version and push it to the remote repository. No CI build will launch when you run the following commands. This is because there is no trigger for CI to run when tags are pushed to the remote repository.\ngit tag \u0026lt;RELEASE_VERSION\u0026gt; git push origin \u0026lt;RELEASE_VERSION\u0026gt; 7. Create the GitHub release Navigate to the repository\u0026rsquo;s GitHub Tags page. Create a new release from the tag you created in the previous step.\n      This will take you to a page where you can draft the release. Fill out the release form.\n Add the \u0026lt;RELEASE_VERSION\u0026gt; to the Release title field. Add the change entries you pasted into the CHANGELOG.md in Step 4 to the Release description field.  Then click the Publish release button.\nWARNING: This will trigger GitHub Actions to build and publish the release binaries!\n      Monitor the GitHub Actions to confirm the release is successful.\n8. Forward merge release changes to master Apply the commits in the release to the master branch. To do this, you will need to know the SHA of each commit. You can get these from the CHANGELOG.md.\nBLOCK_START=$(grep -n -m 1 \u0026quot;###\u0026quot; CHANGELOG.md \\ | cut -d':' -f1) BLOCK_END=$(grep -n -m 2 \u0026quot;###\u0026quot; CHANGELOG.md \\ | cut -d':' -f1 | tail -n 1) head -n $((${BLOCK_END}-1)) CHANGELOG.md \\ | tail -n $((${BLOCK_END}-${BLOCK_START}-1)) \\ | sed 's/^.*\\/\\([a-f0-9]*\\).*/\\1/g' Prepare to merge the commits into the master branch\ngit checkout master git merge \u0026lt;RELEASE_BRANCH\u0026gt; -s ours Cherry-pick each SHA in the above list of commits one at a time. Make any manual adjustments required as you apply each commit to the master branch. Push the changes to the remote repository.\ngit cherry-pick -n \u0026lt;SHA\u0026gt; git commit -a --amend git push origin master "
},
{
	"uri": "http://cassandra-reaper.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://cassandra-reaper.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]